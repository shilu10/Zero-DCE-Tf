{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport cv2\nimport time \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as ans \nfrom tqdm import tqdm \nimport shutil \nimport tensorflow as tf \nfrom tensorflow import keras \nfrom tensorflow.keras import Model\nfrom tensorflow.keras import layers \nfrom tensorflow.keras.layers import *\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.initializers import RandomNormal\nfrom tensorflow.keras import *\nfrom datetime import datetime\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.layers import Dense, Input, UpSampling2D, Conv2DTranspose, Conv2D, add, Add,\\\n                    Lambda, Concatenate, AveragePooling2D, BatchNormalization, GlobalAveragePooling2D, \\\n                    Add, LayerNormalization, Activation, LeakyReLU, SeparableConv2D\ntry:\n    import tensorflow_addons as tfa \nexcept:\n    !pip install tensorflow_addons\n    import tensorflow_addons as tfa\n    from tensorflow_addons.layers import InstanceNormalization\ntry:\n    import gdown \nexcept:\n    !pip install gdown --quiet\n    import gdown\n    \ntry: \n    from imutils import paths \nexcept:\n    !pip install imutils  --quiet\n    from imutils import paths\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport PIL\nimport glob\nfrom PIL import Image\nimport cv2\nimport collections \nfrom collections import *","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-01T03:26:12.876459Z","iopub.execute_input":"2023-06-01T03:26:12.876883Z","iopub.status.idle":"2023-06-01T03:26:17.240197Z","shell.execute_reply.started":"2023-06-01T03:26:12.876851Z","shell.execute_reply":"2023-06-01T03:26:17.239052Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"class DataLoader: \n    \"\"\"\n        Class, will be useful for creating the BYOL dataset or dataset for the DownStream task \n            like classification or segmentation.\n        Methods:\n            __download_data(scope: private)\n            __normalize(scope: private)\n            __preprocess_img(scope: private)\n             __get_val_data(scope: private)\n            get_dataset(scope: public)\n        \n        Property:\n            dname(dtype: str)        : dataset name(supports cifar10, cifar100).\n            augmentor(type           : byol augmentor instance/object.\n            nval(type: int)          : Number of validation data needed, this will be created by splitting the testing\n                                       data.\n            resize_shape(dtype: int) : Resize shape, bcoz pretrained models, might have a different required shape. If Resize shape is 0\n                                        no resize, will happen.\n            normalize(dtype: bool)   : bool value, whether to normalize the data or not. \n    \"\"\"\n    \n    def __init__(self, dname=\"cifar10\", augmentor=None, nval=5000, resize_shape=0, normalize=True): \n        assert dname in [\"cifar10\", 'cifar100'], \"dname should be either cifar10 or cifar100\"\n        assert nval <= 10_000, \"ValueError: nval value should be <= 10_000\"\n        \n        __train_data, __test_data = self.__download_data(dname)\n        self.__train_X, self.__train_y = __train_data\n        self.__train_X, self.__train_y = self.__train_X, self.__train_y\n        self.__dtest_X, self.__dtest_y = __test_data \n        self.__dname = dname\n        \n        self.augmentor = augmentor\n        self.__get_val_data(nval)\n        self.resize_shape = resize_shape\n        \n        \n        self.__normalize() if normalize else None\n        \n    def __len__(self): \n        return self.__train_X.shape[0] + self.__dtest_X.shape[0]\n    \n    def __repr__(self): \n        return f\"Training Samples: {self.__train_X.shape[0]}, Testing Samples: {self.__dtest_X.shape[0]}\"\n    \n    def __download_data(self, dname):\n        \"\"\"\n            Downloads the data from the tensorflow website using the tensorflw.keras.load_data() method.\n            Params:\n                dname(type: Str): dataset name, it just supports two dataset cifar10 or cifar100\n            Return(type(np.ndarray, np.ndarray))\n                returns the training data and testing data\n        \"\"\"\n        if dname == \"cifar10\": \n            train_data, test_data = tf.keras.datasets.cifar10.load_data()\n        if dname == \"cifar100\": \n            train_data, test_data = tf.keras.datasets.cifar100.load_data()\n            \n        return train_data, test_data\n    \n    def __normalize(self): \n        \"\"\"\n            this method, will used to normalize the inputs.\n        \"\"\"\n        self.__train_X = self.__train_X / 255.0\n        self.__dtest_X = self.__dtest_X / 255.0\n    \n    def __preprocess_img(self, image, label, augment=False): \n        \"\"\"\n            this method, will be used by the get_byol_dataset methos, which does a convertion of \n            numpy data to tensorflow data.\n            Params:\n                image(type: np.ndarray): image data.\n            Returns(type; (np.ndarray, np.ndarray))\n                returns the two different augmented views of same image.\n        \"\"\"\n        try: \n            image = tf.cast(image, tf.float32)\n            if self.__dname == 'cifar10':\n                print(label.shape)\n                label = tf.one_hot(label, 10)\n                label = tf.reshape(label, (label.shape[0], 10))\n             #   label = tf.reshape(label, 10)\n            else:\n                label = tf.one_hot(label, 100)\n                label = tf.reshape(label, (label.shape[0], 100))\n               # label = tf.reshape(label, 100)\n            \n            if self.resize_shape > 0:\n                image = tf.image.resize(image, (self.resize_shape, self.resize_shape))\n         \n            if self.augmentor and augment:\n                image = self.paugmentor.augment(image)\n           \n            return image, label\n        \n        except Exception as err:\n            return err\n    \n    def get_dataset(self, batch_size, dataset_type=\"train\"):\n        \"\"\"\n            this method, will gives the byol dataset, which is nothing but a tf.data.Dataset object.\n            Params:\n                batch_size(dtype: int)    : Batch Size.\n                dataset_type(dtype: str)  : which type of dataset needed, (train, test or val)\n                \n            return(type: tf.data.Dataset)\n                returns the tf.data.Dataset for intended dataset_type, by preprocessing and converting \n                the np data.\n        \"\"\"\n        try:\n            if dataset_type == \"train\":\n                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__train_X, self.__train_y))\n                tensorflow_data = (\n                tensorflow_data\n                    .shuffle(1024)\n                    .batch(batch_size, drop_remainder=True)\n                    .map(lambda x, label: (self.__preprocess_img(x, label,  augment=True)),\n                                                 num_parallel_calls=tf.data.AUTOTUNE).cache()\n                    \n                    .prefetch(tf.data.experimental.AUTOTUNE)\n                )\n                return tensorflow_data  \n            \n            if dataset_type == \"test\":\n                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__test_X, self.__test_y))\n                tensorflow_data = (\n                tensorflow_data\n                    .shuffle(1024)\n                    .map(lambda x, label: (self.__preprocess_img(x, label, augment=False)),\n                                                 num_parallel_calls=tf.data.AUTOTUNE).cache()\n                    .batch(batch_size, drop_remainder=True)\n                    .prefetch(tf.data.experimental.AUTOTUNE)\n                )\n                return tensorflow_data  \n            \n            if dataset_type == \"val\":\n                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__val_X, self.__val_y))\n                tensorflow_data = (\n                tensorflow_data\n                     .shuffle(1024)\n                    .map(lambda x, label: (self.__preprocess_img(x, label, augment=False)),\n                                                 num_parallel_calls=tf.data.AUTOTUNE).cache()\n                    .batch(batch_size, drop_remainder=True)\n                    .prefetch(tf.data.experimental.AUTOTUNE)\n                )\n                return tensorflow_data  \n        \n        except Exception as err:\n            return err\n    \n   \n    def __get_val_data(self, nval):\n        \"\"\"\n            this method is used to create a validation data by randomly sampling from the testing data.\n            Params:\n                nval(dtype: Int); Number of validation data needed, rest of test_X.shape[0] - nval, will be \n                                  testing data size.\n            returns(type; np.ndarray, np.ndarray):\n                returns the testing and validation dataset.\n        \"\"\"\n        try: \n            ind_arr = np.arange(10_000)\n            val_inds = np.random.choice(ind_arr, nval, replace=False)\n            test_inds = [i for i in ind_arr if not i in val_inds]\n\n            self.__test_X, self.__test_y = self.__dtest_X[test_inds], self.__dtest_y[test_inds]\n            self.__val_X, self.__val_y = self.__dtest_X[val_inds], self.__dtest_y[val_inds]\n            \n        except Exception as err:\n            raise err    \n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:26:17.242982Z","iopub.execute_input":"2023-06-01T03:26:17.243817Z","iopub.status.idle":"2023-06-01T03:26:17.272359Z","shell.execute_reply.started":"2023-06-01T03:26:17.243778Z","shell.execute_reply":"2023-06-01T03:26:17.271068Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_loader = DataLoader(dname=\"cifar100\", augmentor=None, nval=5000, resize_shape=224, normalize=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:26:17.276050Z","iopub.execute_input":"2023-06-01T03:26:17.276431Z","iopub.status.idle":"2023-06-01T03:26:19.144016Z","shell.execute_reply.started":"2023-06-01T03:26:17.276389Z","shell.execute_reply":"2023-06-01T03:26:19.142920Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_ds = data_loader.get_dataset(8, 'train')\ntrain_ds","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:26:19.146277Z","iopub.execute_input":"2023-06-01T03:26:19.146625Z","iopub.status.idle":"2023-06-01T03:26:23.762325Z","shell.execute_reply.started":"2023-06-01T03:26:19.146599Z","shell.execute_reply":"2023-06-01T03:26:23.761224Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"WARNING: AutoGraph could not transform <function DataLoader.get_dataset.<locals>.<lambda> at 0x781f28293d00> and will run it as-is.\nCause: mangled names are not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<_PrefetchDataset element_spec=(TensorSpec(shape=(8, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(8, 100), dtype=tf.float32, name=None))>"},"metadata":{}}]},{"cell_type":"code","source":"val_ds = data_loader.get_dataset(8, 'val')\nval_ds","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:26:23.763841Z","iopub.execute_input":"2023-06-01T03:26:23.764209Z","iopub.status.idle":"2023-06-01T03:26:23.888141Z","shell.execute_reply.started":"2023-06-01T03:26:23.764162Z","shell.execute_reply":"2023-06-01T03:26:23.887082Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"WARNING: AutoGraph could not transform <function DataLoader.get_dataset.<locals>.<lambda> at 0x781fcaf235b0> and will run it as-is.\nCause: mangled names are not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<_PrefetchDataset element_spec=(TensorSpec(shape=(8, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(8, 1, 100), dtype=tf.float32, name=None))>"},"metadata":{}}]},{"cell_type":"code","source":"test_ds = data_loader.get_dataset(8, 'test')\ntest_ds","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:26:23.889534Z","iopub.execute_input":"2023-06-01T03:26:23.889914Z","iopub.status.idle":"2023-06-01T03:26:23.998329Z","shell.execute_reply.started":"2023-06-01T03:26:23.889876Z","shell.execute_reply":"2023-06-01T03:26:23.997197Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"WARNING: AutoGraph could not transform <function DataLoader.get_dataset.<locals>.<lambda> at 0x781f272b2cb0> and will run it as-is.\nCause: mangled names are not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<_PrefetchDataset element_spec=(TensorSpec(shape=(8, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(8, 1, 100), dtype=tf.float32, name=None))>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP(keras.Model):\n    \"\"\"\n        this class is a implementation of the mlp block described in the swin transformer paper, which contains \n        2 fully connected layer with GelU activation.\n    \"\"\"\n    def __init__(self, input_dims=None, hidden_neurons=None,\n                 output_neurons=None, act_type=\"gelu\", dropout_rate=0., prefix=''):\n        \"\"\"\n            Params:\n                input_neurons(dtype: int)   : input dimension for the mlp block, it needed only for .summary() method.\n                hidden_neurons(dtype: int)  : number of neurons in the hidden\n                                              layer(fully connected layer).\n                output_neurons(dtype: iny)  ; number of neurons in the last\n                                              layer(fully connected layer) of mlp.\n                act_type(type: str)         ; type of activation needed. in paper, GeLU is used.\n                dropout_rate(dtype: float)  : dropout rate in the dropout layer.\n                prefix(type: str)           : used for the naming the layers.\n        \"\"\"\n        super(MLP, self).__init__()\n        \n        self.input_dims = input_dims\n        self.fc_1 = Dense(units=hidden_neurons, name=f\"{prefix}/mlp/dense_1\")\n        if act_type == \"relu\":\n            self.act = tf.keras.layers.ReLU()\n        else:\n            self.act = None\n        self.fc_2 = Dense(units=hidden_neurons, name=f\"{prefix}/mlp/dense_2\")\n        self.drop = Dropout(dropout_rate)\n\n    def call(self, x):\n        x = self.fc_1(x)\n        if not self.act:\n            x = tf.keras.activations.gelu(x)\n        else: \n            x = self.act(x)\n        x = self.drop(x)\n        x = self.fc_2(x)\n        x = self.drop(x)\n        return x\n    \n    def summary(self): \n        inputs = Input(shape=self.input_dims)\n        return keras.Model(inputs=inputs, outputs=self.call(inputs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP(tf.keras.layers.Layer):\n    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = Dense(hidden_features, name=f'mlp/fc1')\n        self.fc2 = Dense(out_features, name=f'mlp/fc2')\n        self.drop = Dropout(drop)\n\n    def call(self, x):\n        x = self.fc1(x)\n        x = tf.keras.activations.gelu(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:26:26.535619Z","iopub.execute_input":"2023-06-01T03:26:26.536026Z","iopub.status.idle":"2023-06-01T03:26:26.544695Z","shell.execute_reply.started":"2023-06-01T03:26:26.535998Z","shell.execute_reply":"2023-06-01T03:26:26.543497Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def window_partition(x: tf.Tensor, window_size: int):\n    \"\"\"\n        this function is used to create a local window, with the windo_size.\n        Params:\n            x: (B, H, W, C)\n            window_size (int): window size\n        Returns:\n            windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    # batch height width and channles\n    B, H, W, C = tf.shape(x).numpy()\n    \n    x = tf.reshape(\n        x, (B, H // window_size, window_size, W // window_size, window_size, C)\n    )\n    x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n    windows = tf.reshape(x, shape=[-1, window_size, window_size, C])\n    return windows","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:26:27.115568Z","iopub.execute_input":"2023-06-01T03:26:27.115949Z","iopub.status.idle":"2023-06-01T03:26:27.123629Z","shell.execute_reply.started":"2023-06-01T03:26:27.115919Z","shell.execute_reply":"2023-06-01T03:26:27.122655Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def window_reverse(windows: tf.Tensor, window_size: int, H: int, W: int, C: int):\n    \"\"\"\n    Params:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = tf.shape(windows)[0] // tf.cast(H * W / window_size / window_size, dtype=\"int32\")\n    x = tf.reshape(windows, (B, H // window_size, W // window_size,\n                                             window_size, window_size, -1))\n    x = tf.transpose(x, [0, 1, 3, 2, 4, 5])\n    return tf.reshape(x, (B, H, W, -1))","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:26:28.295734Z","iopub.execute_input":"2023-06-01T03:26:28.296118Z","iopub.status.idle":"2023-06-01T03:26:28.306094Z","shell.execute_reply.started":"2023-06-01T03:26:28.296085Z","shell.execute_reply":"2023-06-01T03:26:28.304629Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_relative_position_index(win_h, win_w):\n    # get pair-wise relative position index for each token inside the window\n    xx, yy = tf.meshgrid(range(win_h), range(win_w))\n    coords = tf.stack([yy, xx], axis=0)  # [2, Wh, Ww]\n    coords_flatten = tf.reshape(coords, [2, -1])  # [2, Wh*Ww]\n\n    relative_coords = (\n        coords_flatten[:, :, None] - coords_flatten[:, None, :]\n    )  # [2, Wh*Ww, Wh*Ww]\n    relative_coords = tf.transpose(\n        relative_coords, perm=[1, 2, 0]\n    )  # [Wh*Ww, Wh*Ww, 2]\n\n    xx = (relative_coords[:, :, 0] + win_h - 1) * (2 * win_w - 1)\n    yy = relative_coords[:, :, 1] + win_w - 1\n    relative_coords = tf.stack([xx, yy], axis=-1)\n\n    return tf.reduce_sum(relative_coords, axis=-1)  # [Wh*Ww, Wh*Ww]\n\n\nclass WindowAttention(layers.Layer):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        head_dim (int): Number of channels per head (dim // num_heads if not set)\n        window_size (tuple[int]): The height and width of the window.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0., prefix=''):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n        self.prefix = prefix\n\n        self.qkv = Dense(dim * 3, use_bias=qkv_bias,\n                         name=f'{self.prefix}/attn/qkv')\n        self.attn_drop = Dropout(attn_drop)\n        self.proj = Dense(dim, name=f'{self.prefix}/attn/proj')\n        self.proj_drop = Dropout(proj_drop)\n\n    def build(self, input_shape):\n        self.relative_position_bias_table = self.add_weight(f'{self.prefix}/attn/relative_position_bias_table',\n                                                            shape=(\n                                                                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1), self.num_heads),\n                                                            initializer=tf.initializers.Zeros(), trainable=True)\n\n        coords_h = np.arange(self.window_size[0])\n        coords_w = np.arange(self.window_size[1])\n        coords = np.stack(np.meshgrid(coords_h, coords_w, indexing='ij'))\n        coords_flatten = coords.reshape(2, -1)\n        relative_coords = coords_flatten[:, :,\n                                         None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.transpose([1, 2, 0])\n        relative_coords[:, :, 0] += self.window_size[0] - 1\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1).astype(np.int64)\n        self.relative_position_index = tf.Variable(initial_value=tf.convert_to_tensor(\n            relative_position_index), trainable=False, name=f'{self.prefix}/attn/relative_position_index')\n        self.built = True\n\n    def call(self, x, mask=None):\n        B_, N, C = x.get_shape().as_list()\n        qkv = tf.transpose(tf.reshape(self.qkv(\n            x), shape=[-1, N, 3, self.num_heads, C // self.num_heads]), perm=[2, 0, 3, 1, 4])\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        q = q * self.scale\n        attn = (q @ tf.transpose(k, perm=[0, 1, 3, 2]))\n        relative_position_bias = tf.gather(self.relative_position_bias_table, tf.reshape(\n            self.relative_position_index, shape=[-1]))\n        relative_position_bias = tf.reshape(relative_position_bias, shape=[\n                                            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1])\n        relative_position_bias = tf.transpose(\n            relative_position_bias, perm=[2, 0, 1])\n        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n\n        if mask is not None:\n            nW = mask.get_shape()[0]  # tf.shape(mask)[0]\n            attn = tf.reshape(attn, shape=[-1, nW, self.num_heads, N, N]) + tf.cast(\n                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), attn.dtype)\n            attn = tf.reshape(attn, shape=[-1, self.num_heads, N, N])\n            attn = tf.nn.softmax(attn, axis=-1)\n        else:\n            attn = tf.nn.softmax(attn, axis=-1)\n\n        attn = self.attn_drop(attn)\n\n        x = tf.transpose((attn @ v), perm=[0, 2, 1, 3])\n        x = tf.reshape(x, shape=[-1, N, C])\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:38.407452Z","iopub.execute_input":"2023-06-01T03:51:38.407939Z","iopub.status.idle":"2023-06-01T03:51:38.443513Z","shell.execute_reply.started":"2023-06-01T03:51:38.407901Z","shell.execute_reply":"2023-06-01T03:51:38.442465Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def drop_path(inputs, drop_prob, is_training):\n    if (not is_training) or (drop_prob == 0.):\n        return inputs\n\n    # Compute keep_prob\n    keep_prob = 1.0 - drop_prob\n\n    # Compute drop_connect tensor\n    random_tensor = keep_prob\n    shape = (tf.shape(inputs)[0],) + (1,) * \\\n        (len(tf.shape(inputs)) - 1)\n    random_tensor += tf.random.uniform(shape, dtype=inputs.dtype)\n    binary_tensor = tf.floor(random_tensor)\n    output = tf.math.divide(inputs, keep_prob) * binary_tensor\n    return output\n\n\nclass DropPath(tf.keras.layers.Layer):\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def call(self, x, training=None):\n        return drop_path(x, self.drop_prob, training)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:39.101776Z","iopub.execute_input":"2023-06-01T03:51:39.102189Z","iopub.status.idle":"2023-06-01T03:51:39.111583Z","shell.execute_reply.started":"2023-06-01T03:51:39.102155Z","shell.execute_reply":"2023-06-01T03:51:39.110585Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SwinTransformerBlock(keras.Model):\n    \"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, input_resolution, num_heads, window_size=7,\n                 shift_size=0, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n                 drop=0., attn_drop=0., drop_path=0., act_layer='gelu',\n                 norm_layer=LayerNormalization): \n    \n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim \n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio \n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n        self.norm1 = norm_layer(epsilon=1e-5, name=f'norm1')\n\n        self.attn = WindowAttention(\n                        dim,\n                        window_size=self.window_size,\n                        num_heads=num_heads,\n                        qkv_bias=qkv_bias,\n                        attn_drop=attn_drop,\n                        proj_drop=drop\n                    )\n\n        self.drop_path = DropPath(\n            drop_path if (drop_path) > 0. else 0.\n        )\n        self.norm2 = norm_layer(epsilon=1e-5, name=f'norm2')\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        self.mlp = MLP(input_dims=None,\n                        hidden_neurons=mlp_hidden_dim,\n                        output_neurons=None,\n                        act_type='gelu',\n                        dropout_rate=drop,\n                        prefix='swin_transformer'\n                      )\n        if self.shift_size > 0:\n                H, W = self.input_resolution\n                img_mask = np.zeros([1, H, W, 1])\n                h_slices = (slice(0, -self.window_size),\n                            slice(-self.window_size, -self.shift_size),\n                            slice(-self.shift_size, None))\n                w_slices = (slice(0, -self.window_size),\n                            slice(-self.window_size, -self.shift_size),\n                            slice(-self.shift_size, None))\n                cnt = 0\n                for h in h_slices:\n                    for w in w_slices:\n                        img_mask[:, h, w, :] = cnt\n                        cnt += 1\n\n                img_mask = tf.convert_to_tensor(img_mask)\n                mask_windows = window_partition(img_mask, self.window_size)\n                mask_windows = tf.reshape(\n                    mask_windows, shape=[-1, self.window_size * self.window_size])\n                attn_mask = tf.expand_dims(\n                    mask_windows, axis=1) - tf.expand_dims(mask_windows, axis=2)\n                attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n                attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n                self.attn_mask = tf.Variable(\n                    initial_value=attn_mask, trainable=False)\n        else:\n            self.attn_mask = None\n\n    def call(self, x, return_attns=False):\n        H, W = self.input_resolution\n        B, L, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n        \n        shortcut = x\n        x = self.norm1(x)\n        x = tf.reshape(x, (B, H, W, C))\n        \n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = tf.roll(\n                x, shift=(-self.shift_size, -self.shift_size), axis=(1, 2)\n            )\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(\n            shifted_x, self.window_size\n        )  # [num_win*B, window_size, window_size, C]\n        x_windows = tf.reshape(\n            x_windows, (-1, self.window_size * self.window_size, C)\n        )  # [num_win*B, window_size*window_size, C]\n\n        # W-MSA/SW-MSA\n        if not return_attns:\n            attn_windows = self.attn(\n                x_windows, mask=self.attn_mask\n            )  # [num_win*B, window_size*window_size, C]\n        else:\n            attn_windows, attn_scores = self.attn(\n                x_windows, mask=self.attn_mask, return_attns=True\n            )  # [num_win*B, window_size*window_size, C]\n        # merge windows\n        attn_windows = tf.reshape(\n            attn_windows, (-1, self.window_size, self.window_size, C)\n        )\n        shifted_x = window_reverse(\n            attn_windows, self.window_size, H, W\n        )  # [B, H', W', C]\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = tf.roll(\n                shifted_x,\n                shift=(self.shift_size, self.shift_size),\n                axis=(1, 2),\n            )\n        else:\n            x = shifted_x\n        x = tf.reshape(x, (B, H * W, C))\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        if return_attns:\n            return x, attn_scores\n        else:\n            return x\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:39.751882Z","iopub.execute_input":"2023-06-01T03:51:39.752821Z","iopub.status.idle":"2023-06-01T03:51:39.787088Z","shell.execute_reply.started":"2023-06-01T03:51:39.752780Z","shell.execute_reply":"2023-06-01T03:51:39.785807Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"class PatchMerging(keras.layers.Layer): \n    \"\"\" Patch Merging Layer.\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, input_resolution, dim, norm_layer=LayerNormalization):\n        super(PatchMerging, self).__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = Dense(2 * dim, use_bias=False, name=f'downsample/reduction')\n        self.norm = norm_layer(epsilon=1e-5, name=f'downsample/norm')\n    \n    def call(self, x): \n        \"\"\"\n        x: B, H * W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.get_shape().as_list()\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = tf.reshape(x, shape=[-1, H, W, C])\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = tf.concat([x0, x1, x2, x3], axis=-1)\n        x = tf.reshape(x, shape=[-1, (H // 2) * (W // 2), 4 * C])\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:40.642046Z","iopub.execute_input":"2023-06-01T03:51:40.642878Z","iopub.status.idle":"2023-06-01T03:51:40.656596Z","shell.execute_reply.started":"2023-06-01T03:51:40.642843Z","shell.execute_reply":"2023-06-01T03:51:40.655470Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"class BasicLayer(keras.Model):\n    \"\"\" A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                         mlp_ratio=4., qkv_bias=True, qk_scale=None,\n                         drop=0., attn_drop=0., drop_path=0., norm_layer=LayerNormalization, downsample=None):\n        super(BasicLayer, self).__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        \n        self.blocks = [\n            SwinTransformerBlock(\n                dim=dim,\n                input_resolution=input_resolution,\n                num_heads=num_heads,\n                window_size=window_size,\n                shift_size=0 if (i % 2 == 0) else window_size // 2,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                drop=drop,\n                attn_drop=attn_drop,\n                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                norm_layer=norm_layer,\n            )\n            for i in range(depth)]\n       \n        # patch merging layer\n        self.downsample=None\n        if downsample != None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        \n    def call(self, x):\n        for blk in self.blocks:\n            x = blk(x)\n        \n        if self.downsample != None:\n            x = self.downsample(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:41.053227Z","iopub.execute_input":"2023-06-01T03:51:41.054432Z","iopub.status.idle":"2023-06-01T03:51:41.066286Z","shell.execute_reply.started":"2023-06-01T03:51:41.054382Z","shell.execute_reply":"2023-06-01T03:51:41.065153Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"class PatchEmbed(keras.layers.Layer):\n    \"\"\" Image to Patch Embedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self, img_size=(224, 224), patch_size=(4, 4), in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        self.img_size = (img_size)\n        self.patch_size = (patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = Conv2D(filters=embed_dim,\n                           kernel_size=patch_size,\n                           strides=patch_size\n                          )\n        if norm_layer is not None:\n            self.norm = norm_layer(epsilon=1e-5, name='norm')\n        else:\n            self.norm = None\n            \n    def call(self, x):\n        B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n      #  assert H == self.img_size[0] and W == self.img_size[1], \\\n       #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        \n        x = self.proj(x)\n        x = tf.reshape(x, shape=[-1, (H // self.patch_size[0]) * (W // self.patch_size[0]), self.embed_dim])\n        if self.norm is not None:\n            x = self.norm(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:41.472066Z","iopub.execute_input":"2023-06-01T03:51:41.472487Z","iopub.status.idle":"2023-06-01T03:51:41.485721Z","shell.execute_reply.started":"2023-06-01T03:51:41.472453Z","shell.execute_reply":"2023-06-01T03:51:41.484538Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SwinTransformer(keras.Model): \n    \"\"\" Swin Transformer\n        A Tensorflow impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        \n    \"\"\"\n    def __init__(self, img_size=(224, 224), patch_size=(4, 4), in_chans=3, num_classes=100,\n                embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0.,\n                attn_drop=0., drop_path_rate=0.1, norm_layer=LayerNormalization, ape=False,\n                patch_norm=False, include_top=True, **kwargs):\n        \n        super(SwinTransformer, self).__init__()\n        \n        self.num_classes=num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n        self.include_top = include_top\n        \n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        \n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n                \n        # absolute postion embedding\n        if self.ape:\n            self.absolute_pos_embed = self.add_weight('absolute_pos_embed',\n                                                      shape=(\n                                                          1, num_patches, embed_dim),\n                                                      initializer=tf.initializers.Zeros())\n\n        self.pos_drop = Dropout(drop_rate)\n        \n        # stochastic depth\n        dpr = [x for x in np.linspace(0., drop_path_rate, sum(depths))]\n        \n        # build layers\n        basic_layers = tf.keras.Sequential([BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                                                input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                                  patches_resolution[1] // (2 ** i_layer)),\n                                                depth=depths[i_layer],\n                                                num_heads=num_heads[i_layer],\n                                                window_size=window_size,\n                                                mlp_ratio=self.mlp_ratio,\n                                                qkv_bias=qkv_bias,\n                                                qk_scale=qk_scale,\n                                                drop=drop_rate,\n                                                attn_drop=attn_drop,\n                                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                                                norm_layer=norm_layer,\n                                                downsample=PatchMerging if (\n                                                    i_layer < self.num_layers - 1) else None) for i_layer in range(self.num_layers)])\n        \n        \n        self.basic_layers = basic_layers\n        self.norm = norm_layer(epsilon=1e-5, name='norm')\n        self.avgpool = GlobalAveragePooling1D()\n        if self.include_top:\n            self.head = Dense(num_classes, name='head')\n        else:\n            self.head = None\n            \n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n    \n        x = self.basic_layers(x)\n        x = self.norm(x)\n        x = self.avgpool(x)\n        return x\n        \n    def call(self, x):\n        x = self.forward_features(x)\n        if self.include_top:\n            x = self.head(x)\n        return x\n        \n  #  def build(self, input_shape):\n   #     x = Input(shape=input_shape)\n    #    return Model(inputs=[x], outputs=self.call(x))\n        \n    def __summary__(self, input_shape):\n        x = Input(shape=input_shape)\n        return Model(inputs=[x], outputs=self.call(x))","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:42.790910Z","iopub.execute_input":"2023-06-01T03:51:42.791333Z","iopub.status.idle":"2023-06-01T03:51:42.814037Z","shell.execute_reply.started":"2023-06-01T03:51:42.791275Z","shell.execute_reply":"2023-06-01T03:51:42.812966Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4.,\n                 qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0., norm_layer=LayerNormalization):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n        \n\n        self.norm1 = norm_layer(epsilon=1e-5, name=f'norm1')\n        self.attn = WindowAttention(\n                        dim,\n                        window_size=(self.window_size, self.window_size),\n                        num_heads=num_heads,\n                        qkv_bias=qkv_bias,\n                        attn_drop=attn_drop,\n                        proj_drop=drop\n                    )\n        self.drop_path = DropPath(\n            drop_path if drop_path > 0. else 0.)\n        self.norm2 = norm_layer(epsilon=1e-5, name=f'norm2')\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        \n        self.mlp = self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim,\n                       drop=drop)\n        #self.mlp = Mlp(input_dims=dim,\n         #               hidden_neurons=mlp_hidden_dim,\n          #              output_neurons=None,\n           #             act_type='gelu',\n            #            dropout_rate=drop,\n             #           prefix='swin_transformer')\n\n    def build(self, input_shape):\n        if self.shift_size > 0:\n            H, W = self.input_resolution\n            img_mask = np.zeros([1, H, W, 1])\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            img_mask = tf.convert_to_tensor(img_mask)\n            mask_windows = window_partition(img_mask, self.window_size)\n            mask_windows = tf.reshape(\n                mask_windows, shape=[-1, self.window_size * self.window_size])\n            attn_mask = tf.expand_dims(\n                mask_windows, axis=1) - tf.expand_dims(mask_windows, axis=2)\n            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n            self.attn_mask = tf.Variable(\n                initial_value=attn_mask, trainable=False, name=f'attn_mask', dtype=tf.float64)\n        else:\n            self.attn_mask = None\n\n        self.built = True\n\n    def call(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.get_shape().as_list()\n\n        shortcut = x\n        x = self.norm1(x)\n        x = tf.reshape(x, shape=[-1, H, W, C])\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = tf.roll(\n                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2])\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)\n        x_windows = tf.reshape(\n            x_windows, shape=[-1, self.window_size * self.window_size, C])\n        \n   #     x_windows = tf.cast(x_windows, dtype=tf.float32)\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n\n        # merge windows\n        attn_windows = tf.reshape(\n            attn_windows, shape=[-1, self.window_size, self.window_size, C])\n        \n        shifted_x = window_reverse(attn_windows, self.window_size, H, W, C)\n        \n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = tf.roll(shifted_x, shift=[\n                        self.shift_size, self.shift_size], axis=[1, 2])\n        \n        else:\n            x = shifted_x\n        x = tf.reshape(x, shape=[-1, H * W, C])\n        \n        # FFN\n        x = shortcut + self.drop_path(x)\n        temp_x = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = temp_x + self.drop_path(x)\n\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:43.803661Z","iopub.execute_input":"2023-06-01T03:51:43.804060Z","iopub.status.idle":"2023-06-01T03:51:43.828807Z","shell.execute_reply.started":"2023-06-01T03:51:43.804028Z","shell.execute_reply":"2023-06-01T03:51:43.827740Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"m = SwinTransformer()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:44.227139Z","iopub.execute_input":"2023-06-01T03:51:44.228626Z","iopub.status.idle":"2023-06-01T03:51:44.435465Z","shell.execute_reply.started":"2023-06-01T03:51:44.228579Z","shell.execute_reply":"2023-06-01T03:51:44.434266Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"m(np.zeros((1, 224, 224, 3)))","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:46.711669Z","iopub.execute_input":"2023-06-01T03:51:46.712056Z","iopub.status.idle":"2023-06-01T03:51:47.563371Z","shell.execute_reply.started":"2023-06-01T03:51:46.712026Z","shell.execute_reply":"2023-06-01T03:51:47.562339Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"opt = keras.optimizers.Adam()\nloss_func = keras.losses.CategoricalCrossentropy(from_logits=True)\nmetric_func = keras.metrics.CategoricalAccuracy()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:52.541874Z","iopub.execute_input":"2023-06-01T03:51:52.543014Z","iopub.status.idle":"2023-06-01T03:51:52.555578Z","shell.execute_reply.started":"2023-06-01T03:51:52.542969Z","shell.execute_reply":"2023-06-01T03:51:52.554570Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"loss_tracker = keras.metrics.Mean()\ndef train_step(inputs, model, opt, loss_func, metric_func):\n    with tf.GradientTape() as tape:\n        X, y = inputs \n        pred_y = model(X)\n\n        loss_val = loss_func(y, pred_y)\n        \n    params = model.trainable_weights\n    grads = tape.gradient(loss_val, params)\n    opt.apply_gradients(zip(grads, params))\n    \n    loss_tracker.update_state(loss_val)\n    metric_func.update_state(y, pred_y)\n    \n    return loss_tracker.result(), metric_func.result()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:56.281808Z","iopub.execute_input":"2023-06-01T03:51:56.282220Z","iopub.status.idle":"2023-06-01T03:51:56.295123Z","shell.execute_reply.started":"2023-06-01T03:51:56.282180Z","shell.execute_reply":"2023-06-01T03:51:56.294046Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1):\n    for step, training_batch in tqdm(enumerate(train_ds), total=len(train_ds)):\n        res = train_step(training_batch, m, opt, loss_func, metric_func)\n    \n    print(f'Epoch: {epoch}, loss: {res[0]}, acc: {res[1]}')\n    loss_tracker.reset_state()\n    metric_func.reset_state()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T03:51:57.002686Z","iopub.execute_input":"2023-06-01T03:51:57.003361Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 10%|▉         | 621/6250 [07:57<1:08:20,  1.37it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}